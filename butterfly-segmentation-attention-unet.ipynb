{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Imports**","metadata":{}},{"cell_type":"markdown","source":"Bellow are all the imports used in the **Notebook**.","metadata":{}},{"cell_type":"code","source":"# Common\nimport keras\nfrom glob import glob\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom numpy import zeros, random\n\n# Data\nfrom tensorflow.image import resize\nfrom keras.preprocessing.image import load_img, img_to_array\n\n# Data viz\nimport matplotlib.pyplot as plt\n\n# Model\nfrom keras.models import Model, Sequential, load_model\nfrom keras.layers import Conv2D, Conv2DTranspose, concatenate, MaxPool2D, Dropout, BatchNormalization, Layer, Input, add, multiply, UpSampling2D\n\n# Model Viz\nfrom tensorflow.keras.utils import plot_model\n\n# Callback\nfrom keras.callbacks import Callback\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-29T09:00:23.40381Z","iopub.execute_input":"2022-09-29T09:00:23.404292Z","iopub.status.idle":"2022-09-29T09:00:23.41287Z","shell.execute_reply.started":"2022-09-29T09:00:23.404246Z","shell.execute_reply":"2022-09-29T09:00:23.411843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data**","metadata":{}},{"cell_type":"markdown","source":"The **foremost** thing that we need to accomplish is to **load the data**.","metadata":{}},{"cell_type":"code","source":"def load_image(path):\n    img = resize( img_to_array( load_img(path) )/255. , (256,256))\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:04.878949Z","iopub.execute_input":"2022-09-29T08:58:04.879703Z","iopub.status.idle":"2022-09-29T08:58:04.885039Z","shell.execute_reply.started":"2022-09-29T08:58:04.879661Z","shell.execute_reply":"2022-09-29T08:58:04.883742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function takes in the **path of the image** and load it using **keras functions**.","metadata":{}},{"cell_type":"code","source":"root_path = '../input/butterfly-dataset/leedsbutterfly/images/'\nimage_paths = sorted(glob(root_path + f\"*.png\"))\nmask_paths = []\nfor path in image_paths:\n    mask_path = path.replace('images','segmentations')\n    mask_path = mask_path.replace('.png','_seg0.png')\n    mask_paths.append(mask_path)\nprint(f\"Total Number of Images  : {len(image_paths)}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:04.887049Z","iopub.execute_input":"2022-09-29T08:58:04.887876Z","iopub.status.idle":"2022-09-29T08:58:05.011146Z","shell.execute_reply.started":"2022-09-29T08:58:04.88784Z","shell.execute_reply":"2022-09-29T08:58:05.009764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This way, by **replacing the text** from the **path** we can easily get the **exact segmentation mask** for a **particular image**.","metadata":{}},{"cell_type":"code","source":"images = zeros(shape=(len(image_paths), 256, 256, 3))\nmasks = zeros(shape=(len(image_paths), 256, 256, 3))\nfor n, (img_path, mask_path) in tqdm(enumerate(zip(image_paths, mask_paths)), desc=\"Loading\"):\n    images[n] = load_image(img_path)\n    masks[n] = load_image(mask_path)","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:05.014299Z","iopub.execute_input":"2022-09-29T08:58:05.015568Z","iopub.status.idle":"2022-09-29T08:58:45.65622Z","shell.execute_reply.started":"2022-09-29T08:58:05.015518Z","shell.execute_reply":"2022-09-29T08:58:45.654911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, our **images and masks** are loaded. It's time to **visualize them. So that we can gain some insights about the data.","metadata":{}},{"cell_type":"markdown","source":"# **Data Visualization**","metadata":{}},{"cell_type":"code","source":"def show_image(image, title=None, alpha=1):\n    plt.imshow(image, alpha=alpha)\n    plt.title(title)\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:45.657937Z","iopub.execute_input":"2022-09-29T08:58:45.65846Z","iopub.status.idle":"2022-09-29T08:58:45.667457Z","shell.execute_reply.started":"2022-09-29T08:58:45.658413Z","shell.execute_reply":"2022-09-29T08:58:45.665947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **below function** will **plot the mask** for us, **with various variations** and we can also use it as a **callback**.","metadata":{}},{"cell_type":"code","source":"def show_mask(GRID, fig_size=(8,20), model=None, join=False, alpha=0.5):\n    \n    # Config the GRID\n    n_rows, n_cols = GRID\n    n_images = n_rows * n_cols\n    n = 1\n    plt.figure(figsize=fig_size)\n    for i in range(1,n_images+1):\n        \n        if model is None:\n\n            if join:\n                \n                # Seect a Random Image and mask\n                id = random.randint(len(images))\n                image, mask = images[id], masks[id]\n                \n                # plot the Mask over the Image\n                plt.subplot(n_rows, n_cols, i)\n                show_image(image)\n                show_image(mask, alpha=alpha)\n                \n            else:\n                \n                if i%2==0:\n                    plt.subplot(n_rows,n_cols,i)\n                    show_image(mask)\n                \n                else:\n                    # Seect a Random Image and mask\n                    id = random.randint(len(images))\n                    image, mask = images[id], masks[id]\n                    \n                    # Plot Image\n                    plt.subplot(n_rows,n_cols,i)\n                    show_image(image)\n        else:\n            if join:\n                \n                if i%2==0:\n                    # plot the Mask over the Image\n                    plt.subplot(n_rows, n_cols, i)\n                    show_image(image)\n                    show_image(pred_mask, alpha=alpha, title=\"Predicted Mask\")\n                else:\n                    # Seect a Random Image and mask\n                    id = random.randint(len(images))\n                    image, mask = images[id], masks[id]\n                    pred_mask = model.predict(tf.expand_dims(image, axis=0))[0]\n                    \n                    # plot the Mask over the Image\n                    plt.subplot(n_rows, n_cols, i)\n                    show_image(image)\n                    show_image(mask, alpha=alpha, title=\"Original Mask\")\n            else:\n                if n==1:\n                    # Seect a Random Image and mask\n                    id = random.randint(len(images))\n                    image, mask = images[id], masks[id]\n                    pred_mask = model.predict(tf.expand_dims(image, axis=0))[0]\n                    \n                    # plot the Mask over the Image\n                    plt.subplot(n_rows, n_cols, i)\n                    show_image(image, title=\"Original Image\")\n                    n+=1\n                    \n                elif n==2:\n                    # plot the Mask over the Image\n                    plt.subplot(n_rows, n_cols, i)\n                    show_image(mask, title=\"Original Mask\")\n                    n+=1\n                elif n==3:\n                    # plot the Mask over the Image\n                    plt.subplot(n_rows, n_cols, i)\n                    show_image(pred_mask, title=\"Predicted Mask\")\n                    n=1\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-29T09:10:33.290495Z","iopub.execute_input":"2022-09-29T09:10:33.290924Z","iopub.status.idle":"2022-09-29T09:10:33.309469Z","shell.execute_reply.started":"2022-09-29T09:10:33.29089Z","shell.execute_reply":"2022-09-29T09:10:33.307743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GRID = [5,4]\nshow_mask(GRID, fig_size=(15,20))","metadata":{"execution":{"iopub.status.busy":"2022-09-29T09:10:35.288487Z","iopub.execute_input":"2022-09-29T09:10:35.288891Z","iopub.status.idle":"2022-09-29T09:10:36.968661Z","shell.execute_reply.started":"2022-09-29T09:10:35.288858Z","shell.execute_reply":"2022-09-29T09:10:36.96754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This can be a **tough task** for the model because the **image background** contains **a lot of objects**. Thus, using an **Attention UNet** would be a good idea.","metadata":{}},{"cell_type":"code","source":"GRID = [5,4]\nshow_mask(GRID, fig_size=(15,20), join=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:47.320786Z","iopub.execute_input":"2022-09-29T08:58:47.321384Z","iopub.status.idle":"2022-09-29T08:58:49.453114Z","shell.execute_reply.started":"2022-09-29T08:58:47.321348Z","shell.execute_reply":"2022-09-29T08:58:49.451556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the mask over the image gives us a **better visualization**.","metadata":{}},{"cell_type":"markdown","source":"# **Attention UNet - Encoder**","metadata":{}},{"cell_type":"markdown","source":"* The **main task** of the **Encoder** is to **downsample the images** by a **factor of 2**, but at the same time **learn the features** present in the image. \n\n* The idea behind encoder is that it will gradually learn all the **useful features** and preserve them in a **latent representation**, which is present in the **last encoding layer**.\n\n* A **small amount of dropout** is also added between the **convolutional layers** in the encoder so that each **layer is forced to learn the most useful features**.","metadata":{}},{"cell_type":"code","source":"class Encoder(Layer):\n    \n    def __init__(self, filters, rate, pooling=True, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        \n        self.filters = filters\n        self.rate = rate\n        self.pooling = pooling\n        \n        self.c1 = Conv2D(filters, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal', activation='relu')\n        self.drop = Dropout(rate)\n        self.c2 = Conv2D(filters, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal', activation='relu')\n        self.pool = MaxPool2D()\n        \n    def call(self, X):\n        x = self.c2(self.drop(self.c1(X)))\n        if self.pooling:\n            y = self.pool(x)\n            return y, x\n        else:\n            return x\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {\n            **base_config,\n            \"filters\":self.filters,\n            \"rate\":self.rate,\n            \"pooling\":self.pooling,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:49.454606Z","iopub.execute_input":"2022-09-29T08:58:49.455876Z","iopub.status.idle":"2022-09-29T08:58:49.466281Z","shell.execute_reply.started":"2022-09-29T08:58:49.455828Z","shell.execute_reply":"2022-09-29T08:58:49.4653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Attention UNet - Decoder**","metadata":{}},{"cell_type":"markdown","source":"* The **decoder** is just the **opposite** of the **encoder** in terms of **functioning** because it **Upsamples** the **input images** or the **input feature maps** by a **factor of 2**.\n\n* The input to **the decoder** are the **latent representations** learned by the encoder. This means the **decoder** has access only to the **most useful features** and it **tries to replicate the segmentation mask** from these features.\n\n* One **major reason** behind the **success of UNet architecture** are the **skip connections** from the **encoder to the decoder layer**. This allowed the **decoder to learn** the **spatial information** present in the original image.|","metadata":{}},{"cell_type":"code","source":"class Decoder(Layer):\n    \n    def __init__(self, filters, rate, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        \n        self.filters = filters\n        self.rate = rate\n        \n        self.cT = Conv2DTranspose(filters, kernel_size=3, strides=2, padding='same', kernel_initializer='he_normal', activation='relu')\n        self.net = Encoder(filters, rate, pooling=False)\n        \n    def call(self, X):\n        x, skip_x = X\n        x = self.cT(x)\n        \n        c = concatenate([x, skip_x])\n        f = self.net(c)\n        return f\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {\n            **base_config,\n            \"filters\":self.filters,\n            \"rate\":self.rate\n        }","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:53.211301Z","iopub.execute_input":"2022-09-29T08:58:53.211755Z","iopub.status.idle":"2022-09-29T08:58:53.221113Z","shell.execute_reply.started":"2022-09-29T08:58:53.211718Z","shell.execute_reply":"2022-09-29T08:58:53.220056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Attention UNet - Attention Gate**","metadata":{}},{"cell_type":"markdown","source":"The **idea behind the attention gate** is to add a **particular gate or a layer** between the **skip connections** so that the **skip connections can be refined** and only the **most important spatial information is fed to the decoder**.","metadata":{}},{"cell_type":"code","source":"class AttentionGate(Layer):\n    \n    def __init__(self, filters, **kwargs):\n        super(AttentionGate, self).__init__(**kwargs)\n        \n        self.filters = filters\n        \n        self.normal = Conv2D(filters, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal', activation='relu')\n        self.down = Conv2D(filters, kernel_size=3, strides=2, padding='same', kernel_initializer='he_normal', activation='relu')\n        \n        self.learn = Conv2D(1, kernel_size=1, strides=1, activation='sigmoid')\n        self.resample = UpSampling2D()\n    \n    def call(self, X):\n        x, skip_x = X\n        \n        x = self.normal(x)\n        skip = self.down(skip_x)\n        a = add([x, skip])\n        \n        l = self.learn(a)\n        l = self.resample(l)\n        \n        f = multiply([l, skip_x])\n        return f\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {\n            **base_config,\n            \"filters\":self.filters\n        }","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:54.297901Z","iopub.execute_input":"2022-09-29T08:58:54.298339Z","iopub.status.idle":"2022-09-29T08:58:54.309564Z","shell.execute_reply.started":"2022-09-29T08:58:54.298305Z","shell.execute_reply":"2022-09-29T08:58:54.308289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Attention UNet**","metadata":{}},{"cell_type":"markdown","source":"So the **Encoder, Decoder and the Attention Gate** is ready. It's time to combine all of them in complete our **Attention Unet** architecture.","metadata":{}},{"cell_type":"code","source":"# Inputs\nimage_input = Input(shape=(256,256,3), name=\"InputImage\")\n\n# Encoder Phase\np1, c1 = Encoder(32, 0.1, name=\"EncoderBlock1\")(image_input)\np2, c2 = Encoder(64, 0.1, name=\"EncoderBlock2\")(p1)\np3, c3 = Encoder(128, 0.2, name=\"EncoderBlock3\")(p2)\np4, c4 = Encoder(256, 0.2, name=\"EncoderBlock4\")(p3)\n\n# Latent Representation\nencoding = Encoder(512, 0.3, pooling=False, name=\"Encoding\")(p4)\n\n# Deocder + Attention Phase\na1 = AttentionGate(256, name=\"Attention1\")([encoding, c4])\nd1 = Decoder(256, 0.2, name=\"DecoderBlock1\")([encoding, a1])\n\na2 = AttentionGate(128, name=\"Attention2\")([d1, c3])\nd2 = Decoder(128, 0.2, name=\"DecoderBlock2\")([d1, a2])\n\na3 = AttentionGate(64, name=\"Attention3\")([d2, c2])\nd3 = Decoder(64, 0.2, name=\"DecoderBlock3\")([d2, a3])\n\na4 = AttentionGate(32, name=\"Attention4\")([d3, c1])\nd4 = Decoder(32, 0.1, name=\"DecoderBlock4\")([d3, a4])\n\n# Output Layer\nmask_out = Conv2D(3, kernel_size=1, strides=1, activation='sigmoid', padding='same', name=\"MaskOut\")(d4)\n\n# Model\natt_unet = Model(\n    inputs=[image_input], outputs=[mask_out], name=\"AttentionUNet\"\n)\n\n# Compile\natt_unet.compile(\n    loss='binary_crossentropy',\n    optimizer='adam'\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:54.964409Z","iopub.execute_input":"2022-09-29T08:58:54.964835Z","iopub.status.idle":"2022-09-29T08:58:56.090474Z","shell.execute_reply.started":"2022-09-29T08:58:54.964801Z","shell.execute_reply":"2022-09-29T08:58:56.089105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Attention UNet - Visualization**","metadata":{}},{"cell_type":"code","source":"plot_model(att_unet, \"AttentionUNet.png\", show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:56.092503Z","iopub.execute_input":"2022-09-29T08:58:56.092888Z","iopub.status.idle":"2022-09-29T08:58:57.811709Z","shell.execute_reply.started":"2022-09-29T08:58:56.092853Z","shell.execute_reply":"2022-09-29T08:58:57.810452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Custom Callback**","metadata":{}},{"cell_type":"markdown","source":"It will be a **good idea to visualize models performance after each epoch.**","metadata":{}},{"cell_type":"code","source":"class ShowProgress(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        show_mask(GRID=[1,1], model=self.model, join=False, fig_size=(20,8))\n        self.model.save(\"AttentionUnet.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:58:57.813918Z","iopub.execute_input":"2022-09-29T08:58:57.814535Z","iopub.status.idle":"2022-09-29T08:58:57.821628Z","shell.execute_reply.started":"2022-09-29T08:58:57.814492Z","shell.execute_reply":"2022-09-29T08:58:57.820239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"markdown","source":"**Training Attention UNet** is simple. Just train it like we train other models.","metadata":{}},{"cell_type":"code","source":"# att_unet.fit(\n#     images, masks,\n#     validation_split=0.1,\n#     epochs=20, \n#     callbacks=[ShowProgress()]\n# )","metadata":{"execution":{"iopub.status.busy":"2022-09-29T09:10:51.332654Z","iopub.execute_input":"2022-09-29T09:10:51.333909Z","iopub.status.idle":"2022-09-29T09:10:51.340019Z","shell.execute_reply.started":"2022-09-29T09:10:51.333839Z","shell.execute_reply":"2022-09-29T09:10:51.338345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluation**","metadata":{}},{"cell_type":"code","source":"att_unet = load_model('../input/attention-unet-butterfly-segmentation/AttentionUnet.h5', custom_objects={\n    \"Encoder\":Encoder,\n    \"Decoder\":Decoder,\n    \"AttentionGate\":AttentionGate\n})","metadata":{"execution":{"iopub.status.busy":"2022-09-29T09:10:52.855612Z","iopub.execute_input":"2022-09-29T09:10:52.85654Z","iopub.status.idle":"2022-09-29T09:10:55.427939Z","shell.execute_reply.started":"2022-09-29T09:10:52.856498Z","shell.execute_reply":"2022-09-29T09:10:55.42668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_mask(GRID=[10,6], model=att_unet, join=False, fig_size=(20,30))","metadata":{"execution":{"iopub.status.busy":"2022-09-29T09:12:31.161338Z","iopub.execute_input":"2022-09-29T09:12:31.161793Z","iopub.status.idle":"2022-09-29T09:12:42.62718Z","shell.execute_reply.started":"2022-09-29T09:12:31.161751Z","shell.execute_reply":"2022-09-29T09:12:42.625885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_mask(GRID=[10,6], model=att_unet, join=True, fig_size=(20,30), alpha=0.8)","metadata":{"execution":{"iopub.status.busy":"2022-09-29T09:18:20.77566Z","iopub.execute_input":"2022-09-29T09:18:20.776129Z","iopub.status.idle":"2022-09-29T09:18:36.352438Z","shell.execute_reply.started":"2022-09-29T09:18:20.776092Z","shell.execute_reply":"2022-09-29T09:18:36.351023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$Observations :$\n\n* During the **initial stage** of **training**, the **model's prediction** will be **very bad**. This is because the **attention model** is still trying to capture or learn the **important spatial information**.\n\n* The **generated mask** for **white butterflies** are **not good**. One possible reason for that is **the attention part of the model** has learned to ignore the **white flowers** present in the image and consider them as flowers. Now it's a theory, it could be wrong.\n\n* I also tried to **add more decoder and encoder** layers, but then it was going out of **RAM**.\n\n* The generated Mask will always be a little blurry. It is **always a good idea to add a post processing function.**\n\n* Overall the **models prediction are very satisfying**.\n\n**Thanks!!**\n\n---\n**DeepNets**","metadata":{}}]}